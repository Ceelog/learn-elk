### INPUTS
input {
    kafka  {
      codec => "json"
      topics_pattern => "elk-nginx-log"
      bootstrap_servers => "kafka:9092"
      auto_offset_reset => "latest"
      group_id => "nginx-log"
    }
}

### FILTERS
filter {

    grok {

      #获取 Nginx 日志字段
      match => {
          "message" => [
          #Nginx access log 格式
          # 172.19.0.1 - - [21/May/2020:09:34:14 +0000] "GET /index.html?f=hello HTTP/1.1" 200 5 0.000 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.92 Safari/537.36" "-"
          '%{IPV4:ip} - - \[%{HTTPDATE:[@metadata][timestamp]}\] "%{WORD:method} %{URIPATH:path}(?:%{GREEDYDATA:query}|) HTTP/%{NUMBER}" %{NUMBER:status} (?:%{NUMBER:body_bytes_sent}) (?:%{NUMBER:request_time}) %{QS:http_referer} %{QS:http_user_agent} %{QS:http_x_forwarded_for}'
          ]
      }

      remove_field => ["message"]
    }
    
    if [path] {

      # 过滤日志
      if [path] =~ /\.js|css|jpeg|jpg|png|gif|ico|swf|svg$/ {
        drop {}
      }

      #获取 日志 时间
      date {
          match => [ "[@metadata][timestamp]", "dd/MMM/yyyy:HH:mm:ss Z" ]
          remove_field => ["input_type", "offset", "tags", "beat" ]
      }

      mutate {
          #转换数据类型
          convert => [
              "status" , "integer",
              "body_bytes_sent" , "integer",
              "request_time", "float"
          ]
      }

      if [query] {
        kv {
          prefix => "p_"
          source => "query"
          field_split => "&?"

          #只存储感兴趣参数
          #include_keys => [ "uid", "vn" ]
          remove_field => ["query" ]
        }

        #url 解码
        urldecode {
          all_fields => true
        }
      }
    }
}


### OUTPUTS

output {

  elasticsearch {
    hosts => "elasticsearch:9200"
    index => "logstash-%{[fields][topic]}-%{+YYYY.MM.dd.HH}"
  }
}
